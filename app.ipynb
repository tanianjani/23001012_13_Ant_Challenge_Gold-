{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import matplotlib\n",
    "matplotlib.use('agg') # add this so we can use GUI matlib outside main thread macOS using non-interactive\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Import for flask\n",
    "from flask import Flask, jsonify\n",
    "from flask import request\n",
    "from flasgger import Swagger, LazyJSONEncoder\n",
    "from flasgger import swag_from\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Set Flask's JSON encoder to LazyJSONEncoder\n",
    "app.json_encoder = LazyJSONEncoder\n",
    "\n",
    "# Swagger template with standard strings\n",
    "swagger_template = {\n",
    "    \"info\": {\n",
    "        \"title\": \"Api Documentation for Data Processing and Modeling For Hatespeech on Tweets\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"description\": \"Dokumentasi Api untuk Data Processing dan Modeling Pada Tweets\",\n",
    "    },\n",
    "    \"host\": \"127.0.0.1:5000\",  # You can set the host directly here\n",
    "}\n",
    "# Swagger configuration\n",
    "swagger_config = {\n",
    "    \"headers\": [],\n",
    "    \"specs\": [\n",
    "        {\n",
    "            \"endpoint\": \"docs\",\n",
    "            \"route\": \"/docs.json\",\n",
    "        }\n",
    "    ],\n",
    "    \"static_url_path\": \"/flasgger_static\",\n",
    "    \"swagger_ui\": True,\n",
    "    \"specs_route\": \"/docs/\",\n",
    "}\n",
    "# Initialize Swagger\n",
    "swagger = Swagger(app, template=swagger_template, config=swagger_config)\n",
    "\n",
    "# Import assets data to df\n",
    "df_abusive = pd.read_csv('assets/abusive.csv')\n",
    "df_kamlay = pd.read_csv('assets/new_kamusalay.csv', encoding='latin-1', header=None)\n",
    "df_kamlay.columns=[\"tidak baku\", \"baku\"]\n",
    "\n",
    "\n",
    "# List endpoint apis\n",
    "@swag_from(\"docs/hello_world.yml\", methods=['GET'])\n",
    "@app.route('/', methods=['GET'])\n",
    "def hello_world():\n",
    "    json_response = {\n",
    "        'status_code' : 200,\n",
    "        'description' : \"Hi, Welcome to this Gold Challenge Project\",\n",
    "        'data' : 'Hello World'\n",
    "    }\n",
    "\n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "\n",
    "@swag_from(\"docs/processing_file.yml\", methods=['POST'])\n",
    "@app.route('/text-processing-file', methods=['POST'])\n",
    "def text_processing_file():\n",
    "    global file_df\n",
    "    files_data = request.files\n",
    "    input_file = files_data.get('file')    \n",
    "    # Add data input file to file_df, with spesific setting and rows default:100\n",
    "    file_df = pd.read_csv(input_file, encoding='latin-1', nrows=100)\n",
    "\n",
    "    # ================= Cleansing Part =================\n",
    "    # Use only tweet columns and remove duplicate\n",
    "    file_df = file_df[['Tweet']]\n",
    "    file_df.drop_duplicates(inplace=True)\n",
    "    # Create new column num of char and words from based tweet\n",
    "    file_df['num_of_char'] = file_df['Tweet'].apply(len)\n",
    "    file_df['num_of_words'] = file_df['Tweet'].apply(lambda x: len(x.split()))\n",
    "    # Create function data cleansing with that replace all \n",
    "    # beside numbers words and space, to empty string. \n",
    "    # and using strip() function remove more spaces on first and end of tweet\n",
    "    def data_cleansing(x):\n",
    "        tweet = x\n",
    "        cleaned_tweet = re.sub(r'[^a-zA-Z0-9 ]', '', tweet).strip()\n",
    "        return cleaned_tweet\n",
    "    # Create function to count abusive words that appears per tweet\n",
    "    def data_abusive_cnt(x):\n",
    "        matched_list = []\n",
    "        for i in range(len(df_abusive)):\n",
    "            for j in x.split():\n",
    "                word = df_abusive['ABUSIVE'].iloc[i]\n",
    "                if word==j.lower():\n",
    "                    matched_list.append(word)\n",
    "        return len(matched_list)\n",
    "\n",
    "    # Create new column based on cleanse tweet that using function data cleansing and count data abusive\n",
    "    file_df['cleaned_tweet'] = file_df['Tweet'].apply(lambda x: data_cleansing(x))\n",
    "    file_df['num_of_char_clean'] = file_df['cleaned_tweet'].apply(len)\n",
    "    file_df['num_of_words_clean'] = file_df['cleaned_tweet'].apply(lambda x: len(x.split()))\n",
    "    file_df['num_of_abusive_words'] = file_df['cleaned_tweet'].apply(lambda x: data_abusive_cnt(x))\n",
    "\n",
    "\n",
    "    # ================= Sqlite3 Part, Create and insert db and table =================\n",
    "    # Check if the 'attachments' directory exists, and create it if it doesn't\n",
    "    if not os.path.exists('attachments'):\n",
    "        os.makedirs('attachments')        \n",
    "    \n",
    "    # Create or connect new db gold_ch_project and create new table file_df if not exists\n",
    "    conn = sqlite3.connect('attachments/gold_ch_project.db')\n",
    "    q_create_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS file_df (Tweet varchar(255), num_of_char int, num_of_words int, cleaned_tweet varchar(255), num_of_char_clean int, num_of_words_clean int);\n",
    "    \"\"\"\n",
    "    conn.execute(q_create_table)\n",
    "    conn.commit()\n",
    "\n",
    "    # Check if there's data already stored in table \n",
    "    cursor = conn.execute(\"SELECT COUNT(*) FROM file_df\")\n",
    "    num_rows = cursor.fetchall()\n",
    "    num_rows = num_rows[0][0]    \n",
    "\n",
    "    # Insert data into the 'file_df' table with same name column\n",
    "    # if file_df is not None:\n",
    "    #     file_df.to_sql('file_df', conn, if_exists='append', index=False)\n",
    "    # conn.close()    \n",
    "    #  Loop only if there's no data in num rows\n",
    "    if num_rows == 0:\n",
    "        for i in range(len(file_df)):\n",
    "            tweet = file_df['Tweet'].iloc[i]\n",
    "            num_of_char = int(file_df['num_of_char'].iloc[i])\n",
    "            num_of_words = int(file_df['num_of_words'].iloc[i])\n",
    "            cleaned_tweet = file_df['cleaned_tweet'].iloc[i]\n",
    "            num_of_char_clean = int(file_df['num_of_char_clean'].iloc[i])\n",
    "            num_of_words_clean = int(file_df['num_of_words_clean'].iloc[i])\n",
    "    \n",
    "            q_insertion = \"INSERT INTO file_df (Tweet, num_of_char, num_of_words, cleaned_tweet, num_of_char_clean, num_of_words_clean) values (?,?,?,?,?,?)\"\n",
    "            conn.execute(q_insertion,(tweet,num_of_char,num_of_words,cleaned_tweet,num_of_char_clean,num_of_words_clean))\n",
    "            conn.commit()   \n",
    "    conn.close()    \n",
    "\n",
    "    # ================= Data Visualization =================\n",
    "    \n",
    "    # Visualize numbers of abused words with barplot\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    countplot = sns.countplot(data=file_df, x=\"num_of_abusive_words\")\n",
    "    for p in countplot.patches:\n",
    "        countplot.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2., p.get_height()), ha='center',\n",
    "                        va='center', xytext=(0, 10), textcoords='offset points')\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "    plt.title('Count of Estimated Number of Abusive Words')\n",
    "    plt.xlabel('Estimated Number of Abusive Words')\n",
    "    plt.savefig('attachments/new_countplot.jpeg')\n",
    "\n",
    "    # Visualize num of cleanse words using boxplot\n",
    "    plt.figure(figsize=(20,4))\n",
    "    boxplot = sns.boxplot(data=file_df, x=\"num_of_words_clean\")\n",
    "    print()\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "    plt.title('Number of Words Boxplot (after tweet cleansing)')\n",
    "    plt.xlabel('')\n",
    "    plt.savefig('attachments/new_boxplot.jpeg')    \n",
    "\n",
    "\n",
    "    # Export to new csv after data cleansing\n",
    "    file_df.to_csv(\"attachments/data_cleanse.csv\", index=False)  # Set index=False to exclude the index column\n",
    "\n",
    "        \n",
    "    # Print json response with data cleaned tweet\n",
    "    json_response = {\n",
    "        'status_code' : 200,\n",
    "        'description' : 'File processing',\n",
    "        'data' : list(file_df['cleaned_tweet'])\n",
    "    }\n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
